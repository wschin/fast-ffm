\documentclass[11pt]{article}
\usepackage{amsmath,mathtools}
\usepackage{algorithm}
\usepackage{algpseudocode}

\newcommand{\bsym}[1]{\ensuremath{\boldsymbol{#1}}}
\newcommand{\bw}{\ensuremath{\bsym{w}}}
\newcommand{\bg}{\ensuremath{\bsym{g}}}
\newcommand{\bv}{\ensuremath{\bsym{v}}}
\newcommand{\bx}{\ensuremath{\bsym{x}}}
\newcommand{\bq}{\ensuremath{\bsym{q}}}
\newcommand{\bbr}{\ensuremath{\mathcal R}}
\newcommand{\field}{\ensuremath{\mathcal F}}
\newcommand{\dotprod}[2]{\ensuremath{\left\langle #1,#2 \right\rangle}}
\newcommand{\norm}[1]{\ensuremath{\left\| #1 \right\|}}
\DeclareMathOperator*{\diag}{diag}

\begin{document}
\begin{center}
    {\Large An Improved Stochastic Gradient Method for Training Large-scale Field-aware Factorization Machine}
\end{center}
\begin{center}
    {\bf \large Wei-Sheng Chin}
\end{center}
\begin{center}
{TLC Team, Microsoft, Redmond, WA}\\
\end{center}

\section{Field-aware Factorization Machine}
Assume that we have a feature vector $\bx\in\bbr^n$, $\field(j)$ denotes the field ID of $j$th coordinate in $\bx$, and $m$ is the number of all possible fields.
The output function of field-aware factorization can be written as
\begin{equation*}
    \hat{y} = \dotprod{\bw}{\bx} + \sum_{j=1}^n \sum_{j'=j+1}^n \dotprod{\bv_{j, \field(j')}}{\bv_{j', \field(j)}} x_jx_{j'}
\end{equation*}
or equivalently
\begin{equation}
    \begin{aligned}
        \hat{y} = \dotprod{\bw}{\bx} &+ \sum_{f=1}^m \underbracket[1.5pt][5pt]{\sum_{\substack{j=1 \\ \field(j)=f}}^n \sum_{\substack{j'=1 \\ \field(j') = f \\ j' > j}}^n \dotprod{\bv_{j, f}}{\bv_{j', f}}x_jx_{j'}}_{\text{intra-field interactions in field $f$}} \\
                                 &+ \sum_{f=1}^m\sum_{f'=f+1}^m \underbracket[1.5pt][12pt]{\dotprod{\bq_{f\rightarrow f'}}{\bq_{f'\rightarrow f}}}_{\mathclap{\text{inter-field interactions between field $f$ and $f'$}}},
    \end{aligned}
    \label{eq:FfmOut}
\end{equation}
where $\dotprod{\cdot}{\cdot}$ stands for inner product, $\bw$ is linear coefficient, $\bv_{j, f}$ is the $j$th feature's latent vector in the $f$th field's hidden space, and
\begin{equation*}
    \bq_{f\rightarrow f'} = \sum_{ \substack{j=1 \\ \field(j)=f} }^n \bv_{j, f'} x_j.
\end{equation*}
Note that $\hat{y}$ is actually an abbreviation of $\hat{y}(\bx)$, a function which maps the given feature vector to a real number.
Also, we summarize the derivatives of \eqref{eq:FfmOut} with respect to FFM parameters below for later discussion.
\begin{itemize}
    \item Gradient of $\hat{y}$ with respect to $\bw$: 
        \begin{equation}
            \frac{\partial \hat{y}}{\partial \bw} = \bx
            \label{eq:LinGrad}
        \end{equation}
    \item Gradient of $\hat{y}$ with respect to $\bv_{j, \field(j)}$:
        \begin{equation}
            \frac{\partial \hat{y}}{\partial \bv_{j, \field(j)}} = (\bq_{\field(j) \rightarrow \field(j)} - \bv_{j, \field(j)}x_j) x_j
            \label{eq:IntraGrad}
        \end{equation}
    \item Gradient of $\hat{y}$ with respect to $\bv_{j, k}$ when $k \ne \field(j)$:
        \begin{equation}
            \frac{\partial \hat{y}}{\partial \bv_{j, k}} = \bq_{k\rightarrow \field(j)} x_j.
            \label{eq:InterGrad}
        \end{equation}
\end{itemize}
As you may have observed in \eqref{eq:FfmOut}, FFM is parameterized by $\bw$ and $\bv_{j, f}$, $j=1,\dots,n$ and $f=1,\dots,m$.
To determine those parameters, we consider an empirical risk minimization problem.
If abel-feature pairs, $(y_1,\bx_1),\dots,(y_l,\bx_l)$, are available, our objective function is
\begin{equation}
    \min_{\bw, \bv_{1, 1},\dots,\bv_{n, m}} \quad  \mathcal \sum_{i=1}^l \left( \sum_{\substack{j=1 \\ x_{ij} \ne 0}}^n \left(\frac{\lambda}{2} w_j^2 + \sum_{f=1}^m \frac{\lambda'}{2} \norm{\bv_{j,f}}^2\right) + \xi(\hat{y}_i; y_i) \right),
    \label{eq:Prob}
\end{equation}
where $x_{ij}$ is the the $j$th feature of the $i$th example, $\xi(\hat{y};y)$ is the considered loss function, and $\hat{y}_i=\hat{y}(\bx_i)$.
Note that for binary classification, a common choice is $\xi(\hat{y};y) = \log(1+e^{-y\hat{y}})$, and for regression problems, one may consider $\xi(\hat{y};y) = (\hat{y}-y)^2$.

\section{Stochastic Gradient Methods for Solving \eqref{eq:Prob}}
We consider an advanced stochastic gradient method, ADAGRAD, to solve \eqref{eq:Prob}.
Let
\begin{equation*}
    \xi_i' = \frac{\partial \xi(\hat{y}_i; y_i)}{\partial \hat{y}_i}.
\end{equation*}
With \eqref{eq:LinGrad}, \eqref{eq:IntraGrad}, \eqref{eq:InterGrad}, and chain rule, the $i$th example's gradient can be computed via
\begin{equation}
    g_{w_j} = 
    \begin{cases}
        \lambda w_j + \xi_i' x_{ij} &\text{ if } x_{ij} \ne 0\\
        0 &\text{ otherwise}
    \end{cases}
    \label{eq:GradW}
\end{equation}
and
\begin{equation}
    \bg_{\bv_{j, k}} = 
    \begin{cases}
        \lambda' \bv_{j,\field(j)} + \xi_i' (\bq_{\field(j) \rightarrow \field(j)} - \bv_{j, \field(j)}x_{ij}) x_{ij} & \text{ if } k=\field(j) \text{ and } x_{ij}\ne 0\\
        \lambda' \bv_{j,k} + \xi_i' \bq_{k\rightarrow \field(j)} x_{ij} & \text{ if } k\ne \field(j) \text{ and } x_{ij}\ne 0\\
        \bsym{0} & \text{ otherwise}.
    \end{cases}
    \label{eq:GradV}
\end{equation}
For the sake of simplicity, we drop the example index $i$ if the context is clear.
One our training iteration can be decomposed into two consecutive steps, Algorithm 1 and Algorithm 2.
In the first step, we compute the output value via \eqref{eq:FfmOut}.
The second step calculates the stochastic gradient and then update the model.
Notice that some intermediate variables, $\hat{y}$ and $\bq_{f\rightarrow f'}$, $\forall f,f'\in\{1,\dots,m\}$, obtained in Algorithm 1 can be reused in this step.
Our full algorithm is summarized in Algorithm 3.
\begin{algorithm}
    \begin{algorithmic}[1]
        \State Given model parameters $\bw,\bv_{1,1},\dots,\bv_{n,m}$.
        \State Apply zero initialization to $\hat{y},\bq_{1,1},\dots,\bq_{m,m}$.
        \For { $j = 1,\dots,n$}
            \If { $x_j=0$ }
                \State {\bf continue}
            \EndIf
            \State $\hat{y} \gets \hat{y} + w_jx_j$ \Comment{linear term}
            \For { $f'=1,\dots,m$ }
                \State $\bq_{\field(j)\rightarrow f'} \gets \bq_{\field(j)\rightarrow f'} + \bv_{\field(j), f'} x_j$
            \EndFor
        \EndFor
        \For {$f=1,\dots,m$}
            \State $\hat{y} \gets \hat{y} + \frac{1}{2}\dotprod{\bq_{f\rightarrow f}}{\bq_{f\rightarrow f}}$ \Comment{intra-field interaction}
            \For {$f'=f+1,\dots,m$}
                \State $\hat{y}\gets \hat{y} + \dotprod{\bq_{f\rightarrow f'}}{\bq_{f'\rightarrow f}}$ \Comment{inter-field interaction}
            \EndFor
        \EndFor
        \For { $j=1,\dots,n$}
            \If { $x_j=0$ }
                \State {\bf continue}
            \EndIf
            \State $\hat{y} \gets \hat{y} - \frac{1}{2} \dotprod{\bv_{\field(j),\field(j)}}{\bv_{\field(j),\field(j)}}x_j^2$ \Comment{remove self-interaction}
        \EndFor
    \end{algorithmic}
    \label{alg:FfmStepOne}
    \caption{Evaluation of \eqref{eq:FfmOut}.}
\end{algorithm}

\begin{algorithm}
    \begin{algorithmic}[1]
        \State Given model parameters $\bw,\bv_{1,1},\dots,\bv_{n,m}$, their learning rates $G_1,\dots,G_j,H_{1,1},\dots,H_{n,m}$, and $\bq_{1,1},\dots,\bq_{m,m}$ obtained via Algorithm \ref{alg:FfmStepOne}.
        \State Apply zero initialization to $\hat{y},\bq_{1,1},\dots,\bq_{m,m}$.
        \State Compute $\xi_i'$.
        \For {$j=1,\dots,n$}
            \If {$x_j=0$}
                \State {\bf continue}
            \EndIf
            \State Compute $g_{w_j}$ via \eqref{eq:GradW}.
            \State $G_j \gets G_j + g_{w_j}^2$ \Comment{accumulate squared gradient}
            \State $w_j \gets w_j - \eta G_j^{-\frac{1}{2}} g_{w_j}$ \Comment{ADAGRAD step}
        \EndFor
        \For {$j=1,\dots,n$}
            \If {$x_j=0$}
                \State {\bf continue}
            \EndIf
            \For {$f'=1,\dots,m$}
                \State Compute $\bg_{\bv_{j,f'}}$ via \eqref{eq:GradV}.
                \State $H_{j,f'}  \gets H_{j,f'} + \diag(\bg_{\bv_{j,f'}})^2$ \Comment{accumulate squared gradient}
                \State $\bv_{j,f'} \gets \bv_{j,f'} - \eta H_{j,f'}^{-\frac{1}{2}} \bg_{\bv_{j,f'}}$ \Comment{ADAGRAD step}
            \EndFor
        \EndFor
    \end{algorithmic}
    \label{alg:FfmStepTwo}
    \caption{Update of parameters via stochastic gradient method. We use $\diag(\cdot)$ to denote the diagonal matrix formed by the input vector.}
\end{algorithm}
\begin{algorithm}
    \begin{algorithmic}[1]
        \State Initialize $\bw,\bv_{1,1},\dots,\bv_{n,m}$ with random variables and specify leraning rate scale $\eta$.
        \State Assign one to all learning rates, $G_1,\dots,G_j,H_{1,1},\dots,H_{n,m}$.
        \For {$t=1,\dots,T$}
            \State Sample $(y, \bx)$.
            \State Perform Algorithm 1.
            \State Perform Algorithm 2.
        \EndFor
    \end{algorithmic}
    \label{alg:Main}
    \caption{A $T$-iteration procedure for learning field-aware factorization machine.}
\end{algorithm}

\end{document}
